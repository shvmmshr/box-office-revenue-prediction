{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Multimodal Movie Revenue Prediction System\n",
        "\n",
        "This notebook implements a comprehensive deep learning system that combines:\n",
        "- **Text Analysis**: Movie plot/synopsis using BERT\n",
        "- **Video Analysis**: YouTube trailer processing using ResNet50 CNN\n",
        "- **Audio Analysis**: Trailer audio processing using 1D CNNs\n",
        "- **Multimodal Fusion**: Advanced deep learning combination\n",
        "\n",
        "## Revenue Categories\n",
        "- Disaster (0), Flop (1), Successful (2), Average (3)\n",
        "- Hit (4), Outstanding (5), Superhit (6), Blockbuster (7)\n",
        "\n",
        "## Architecture Overview\n",
        "```\n",
        "Movie Plot â†’ [BERT] â†’ 768-dim features\n",
        "YouTube URL â†’ [Video Processor] â†’ [ResNet50] â†’ 2048-dim features\n",
        "            â†’ [Audio Extractor] â†’ [1D CNN] â†’ 1024-dim features\n",
        "                                            â†“\n",
        "                              [Fusion Network] â†’ 8 Revenue Classes\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 1. Installation and Imports\n",
        "\n",
        "First, let's install the required packages and import all necessary libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio transformers\n",
        "!pip install opencv-python librosa soundfile pytube\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn tqdm\n",
        "!pip install accelerate datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import cv2\n",
        "import librosa\n",
        "from pytube import YouTube\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
        "                           confusion_matrix, classification_report)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning Models\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Audio processing\n",
        "import torchaudio\n",
        "from torchaudio import transforms as audio_transforms\n",
        "\n",
        "# Additional utilities\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import json\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import re\n",
        "import time\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Configuration and Parameters\n",
        "\n",
        "Setting up all the hyperparameters and configuration for our multimodal system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"Configuration class for model parameters\"\"\"\n",
        "    # Data splits\n",
        "    SPLIT_OPTIONS = {\n",
        "        'option1': (0.7, 0.2, 0.1),  # 70-20-10\n",
        "        'option2': (0.75, 0.15, 0.1), # 75-15-10\n",
        "        'option3': (0.8, 0.1, 0.1)    # 80-10-10\n",
        "    }\n",
        "    \n",
        "    # Model parameters\n",
        "    MAX_TEXT_LENGTH = 512\n",
        "    VIDEO_FRAME_SIZE = 224\n",
        "    AUDIO_SAMPLE_RATE = 16000\n",
        "    AUDIO_DURATION = 30  # seconds\n",
        "    \n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 16\n",
        "    LEARNING_RATE = 1e-4\n",
        "    NUM_EPOCHS = 50\n",
        "    PATIENCE = 10\n",
        "    \n",
        "    # Model dimensions\n",
        "    TEXT_EMBEDDING_DIM = 768  # BERT-base\n",
        "    VIDEO_EMBEDDING_DIM = 2048  # ResNet50\n",
        "    AUDIO_EMBEDDING_DIM = 1024\n",
        "    FUSION_DIM = 512\n",
        "    NUM_CLASSES = 8\n",
        "    \n",
        "    # Video processing\n",
        "    FRAMES_PER_VIDEO = 30\n",
        "    VIDEO_DURATION = 60  # seconds to analyze\n",
        "\n",
        "# Label mapping for revenue categories\n",
        "LABEL_MAPPING = {\n",
        "    'Disaster': 0, 'Flop': 1, 'Successful': 2, 'Average': 3,\n",
        "    'Hit': 4, 'Outstanding': 5, 'Superhit': 6, 'Blockbuster': 7\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded successfully!\")\n",
        "print(f\"Number of revenue classes: {Config.NUM_CLASSES}\")\n",
        "print(f\"Data split options: {list(Config.SPLIT_OPTIONS.keys())}\")\n",
        "print(f\"Model will process {Config.FRAMES_PER_VIDEO} frames per video\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. YouTube Video Processing\n",
        "\n",
        "This class handles downloading YouTube trailers and extracting frames and audio for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class YouTubeVideoProcessor:\n",
        "    \"\"\"Handles YouTube video downloading and processing\"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir='temp_videos'):\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Video processor initialized. Output directory: {output_dir}\")\n",
        "    \n",
        "    def extract_video_id(self, url):\n",
        "        \"\"\"Extract YouTube video ID from URL\"\"\"\n",
        "        if 'youtube.com/watch' in url:\n",
        "            return parse_qs(urlparse(url).query).get('v', [None])[0]\n",
        "        elif 'youtu.be' in url:\n",
        "            return url.split('/')[-1].split('?')[0]\n",
        "        elif re.match(r'^[A-Za-z0-9_-]{11}$', url):\n",
        "            return url\n",
        "        return None\n",
        "    \n",
        "    def download_video(self, video_url, video_id):\n",
        "        \"\"\"Download YouTube video\"\"\"\n",
        "        try:\n",
        "            yt = YouTube(video_url)\n",
        "            stream = yt.streams.filter(file_extension='mp4', res='720p').first()\n",
        "            if not stream:\n",
        "                stream = yt.streams.filter(file_extension='mp4').first()\n",
        "            \n",
        "            video_path = os.path.join(self.output_dir, f\"{video_id}.mp4\")\n",
        "            if not os.path.exists(video_path):\n",
        "                print(f\"Downloading video: {video_id}\")\n",
        "                stream.download(output_path=self.output_dir, filename=f\"{video_id}.mp4\")\n",
        "            return video_path\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading video {video_id}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def extract_frames(self, video_path, num_frames=30):\n",
        "        \"\"\"Extract frames from video\"\"\"\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "            \n",
        "            # Extract frames evenly distributed across the video\n",
        "            frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n",
        "            frames = []\n",
        "            \n",
        "            for frame_idx in frame_indices:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "                ret, frame = cap.read()\n",
        "                if ret:\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                    frames.append(frame)\n",
        "            \n",
        "            cap.release()\n",
        "            print(f\"Extracted {len(frames)} frames from video\")\n",
        "            return np.array(frames)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting frames: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def extract_audio(self, video_path):\n",
        "        \"\"\"Extract audio from video\"\"\"\n",
        "        try:\n",
        "            # Extract audio using librosa\n",
        "            audio, sr = librosa.load(video_path, sr=Config.AUDIO_SAMPLE_RATE, \n",
        "                                   duration=Config.AUDIO_DURATION)\n",
        "            print(f\"Extracted audio: {len(audio)} samples at {sr} Hz\")\n",
        "            return audio, sr\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting audio: {e}\")\n",
        "            return None, None\n",
        "\n",
        "# Test the video processor\n",
        "video_processor = YouTubeVideoProcessor()\n",
        "print(\"YouTube Video Processor created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Deep Learning Model Architectures\n",
        "\n",
        "Now let's define our deep learning models for each modality (text, video, audio).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4.1 Text Encoder (BERT-based)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"BERT-based text encoder for movie plots/synopsis\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name='bert-base-uncased', freeze_bert=False):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        print(f\"Loading BERT model: {model_name}\")\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(Config.TEXT_EMBEDDING_DIM, Config.TEXT_EMBEDDING_DIM)\n",
        "        \n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "            print(\"BERT parameters frozen\")\n",
        "        \n",
        "        print(f\"Text encoder initialized with {sum(p.numel() for p in self.parameters()):,} parameters\")\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        encoded = self.fc(pooled_output)\n",
        "        return F.relu(encoded)\n",
        "\n",
        "# Test the text encoder\n",
        "text_encoder = TextEncoder()\n",
        "print(\"âœ… Text Encoder created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4.2 Video Encoder (CNN-based with ResNet50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoEncoder(nn.Module):\n",
        "    \"\"\"CNN-based video encoder for trailer frames\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(VideoEncoder, self).__init__()\n",
        "        print(\"Loading ResNet50 backbone...\")\n",
        "        \n",
        "        # Use pre-trained ResNet50 as backbone\n",
        "        self.backbone = models.resnet50(pretrained=True)\n",
        "        self.backbone.fc = nn.Identity()  # Remove final FC layer\n",
        "        \n",
        "        # Temporal pooling and processing\n",
        "        self.temporal_conv = nn.Conv1d(Config.VIDEO_EMBEDDING_DIM, \n",
        "                                     Config.VIDEO_EMBEDDING_DIM, \n",
        "                                     kernel_size=3, padding=1)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(Config.VIDEO_EMBEDDING_DIM, Config.VIDEO_EMBEDDING_DIM)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        print(f\"Video encoder initialized with {sum(p.numel() for p in self.parameters()):,} parameters\")\n",
        "    \n",
        "    def forward(self, frames):\n",
        "        # frames: (batch_size, num_frames, 3, height, width)\n",
        "        batch_size, num_frames = frames.shape[:2]\n",
        "        \n",
        "        # Reshape for CNN processing\n",
        "        frames = frames.view(-1, 3, Config.VIDEO_FRAME_SIZE, Config.VIDEO_FRAME_SIZE)\n",
        "        \n",
        "        # Extract features for each frame\n",
        "        frame_features = self.backbone(frames)  # (batch*num_frames, 2048)\n",
        "        frame_features = frame_features.view(batch_size, num_frames, -1)\n",
        "        \n",
        "        # Temporal processing\n",
        "        frame_features = frame_features.transpose(1, 2)  # (batch, features, frames)\n",
        "        temporal_features = self.temporal_conv(frame_features)\n",
        "        pooled_features = self.global_pool(temporal_features).squeeze(-1)\n",
        "        \n",
        "        # Final encoding\n",
        "        encoded = self.fc(pooled_features)\n",
        "        encoded = self.dropout(encoded)\n",
        "        return F.relu(encoded)\n",
        "\n",
        "# Test the video encoder\n",
        "video_encoder = VideoEncoder()\n",
        "print(\"âœ… Video Encoder created successfully!\")\n",
        "\n",
        "# Test with dummy data\n",
        "dummy_frames = torch.randn(2, Config.FRAMES_PER_VIDEO, 3, Config.VIDEO_FRAME_SIZE, Config.VIDEO_FRAME_SIZE)\n",
        "with torch.no_grad():\n",
        "    video_features = video_encoder(dummy_frames)\n",
        "print(f\"Video encoder output shape: {video_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4.3 Audio Encoder (1D CNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AudioEncoder(nn.Module):\n",
        "    \"\"\"CNN-based audio encoder for trailer audio\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(AudioEncoder, self).__init__()\n",
        "        print(\"Initializing Audio Encoder with 1D CNNs...\")\n",
        "        \n",
        "        # 1D CNN for audio processing with fixed kernel sizes\n",
        "        # Input: 480000 samples (16000 * 30 seconds)\n",
        "        self.conv1 = nn.Conv1d(1, 64, kernel_size=1024, stride=512)      # Output: ~935\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=16, stride=8)        # Output: ~115  \n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=8, stride=4)        # Output: ~27\n",
        "        self.conv4 = nn.Conv1d(256, 512, kernel_size=4, stride=2)        # Output: ~12\n",
        "        \n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(512, Config.AUDIO_EMBEDDING_DIM)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        print(f\"Audio encoder initialized with {sum(p.numel() for p in self.parameters()):,} parameters\")\n",
        "    \n",
        "    def forward(self, audio):\n",
        "        # audio: (batch_size, audio_length)\n",
        "        audio = audio.unsqueeze(1)  # (batch_size, 1, audio_length)\n",
        "        \n",
        "        # Progressive feature extraction with proper kernel sizes\n",
        "        x = F.relu(self.conv1(audio))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        \n",
        "        # Global pooling and final projection\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "# Test the audio encoder\n",
        "audio_encoder = AudioEncoder()\n",
        "print(\"âœ… Audio Encoder created successfully!\")\n",
        "\n",
        "# Test with dummy data\n",
        "dummy_audio = torch.randn(2, Config.AUDIO_SAMPLE_RATE * Config.AUDIO_DURATION)\n",
        "with torch.no_grad():\n",
        "    audio_features = audio_encoder(dummy_audio)\n",
        "print(f\"Audio encoder output shape: {audio_features.shape}\")\n",
        "\n",
        "# Debug: Show the progression of feature map sizes\n",
        "print(\"\\nðŸ” Audio feature map size progression:\")\n",
        "test_audio = torch.randn(1, Config.AUDIO_SAMPLE_RATE * Config.AUDIO_DURATION)\n",
        "test_audio = test_audio.unsqueeze(1)\n",
        "print(f\"Input: {test_audio.shape}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    x1 = F.relu(audio_encoder.conv1(test_audio))\n",
        "    print(f\"After conv1: {x1.shape}\")\n",
        "    \n",
        "    x2 = F.relu(audio_encoder.conv2(x1))\n",
        "    print(f\"After conv2: {x2.shape}\")\n",
        "    \n",
        "    x3 = F.relu(audio_encoder.conv3(x2))\n",
        "    print(f\"After conv3: {x3.shape}\")\n",
        "    \n",
        "    x4 = F.relu(audio_encoder.conv4(x3))\n",
        "    print(f\"After conv4: {x4.shape}\")\n",
        "    \n",
        "    x5 = audio_encoder.pool(x4).squeeze(-1)\n",
        "    print(f\"After pooling: {x5.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4.4 Multimodal Fusion Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultimodalFusionModel(nn.Module):\n",
        "    \"\"\"Multimodal fusion model combining text, video, and audio\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(MultimodalFusionModel, self).__init__()\n",
        "        print(\"Creating Multimodal Fusion Model...\")\n",
        "        \n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.video_encoder = VideoEncoder()\n",
        "        self.audio_encoder = AudioEncoder()\n",
        "        \n",
        "        # Fusion layers\n",
        "        total_dim = Config.TEXT_EMBEDDING_DIM + Config.VIDEO_EMBEDDING_DIM + Config.AUDIO_EMBEDDING_DIM\n",
        "        print(f\"Total input dimension for fusion: {total_dim}\")\n",
        "        \n",
        "        self.fusion_layers = nn.Sequential(\n",
        "            nn.Linear(total_dim, Config.FUSION_DIM),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(Config.FUSION_DIM, Config.FUSION_DIM // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(Config.FUSION_DIM // 2, Config.NUM_CLASSES)\n",
        "        )\n",
        "        \n",
        "        # Attention mechanism for modality fusion (optional)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=Config.FUSION_DIM, num_heads=8)\n",
        "        self.modality_projection = nn.Linear(total_dim, Config.FUSION_DIM)\n",
        "        \n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Complete multimodal model initialized with {total_params:,} parameters\")\n",
        "    \n",
        "    def forward(self, text_input_ids, text_attention_mask, video_frames, audio):\n",
        "        # Encode each modality\n",
        "        text_features = self.text_encoder(text_input_ids, text_attention_mask)\n",
        "        video_features = self.video_encoder(video_frames)\n",
        "        audio_features = self.audio_encoder(audio)\n",
        "        \n",
        "        # Concatenate features\n",
        "        combined_features = torch.cat([text_features, video_features, audio_features], dim=1)\n",
        "        \n",
        "        # Apply fusion layers\n",
        "        output = self.fusion_layers(combined_features)\n",
        "        return output\n",
        "\n",
        "# Create the complete multimodal model\n",
        "model = MultimodalFusionModel()\n",
        "print(\"âœ… Complete Multimodal Fusion Model created successfully!\")\n",
        "\n",
        "# Show model architecture summary\n",
        "print(\"\\nðŸ“Š Model Architecture Summary:\")\n",
        "print(f\"Text Encoder (BERT): {Config.TEXT_EMBEDDING_DIM}-dim output\")\n",
        "print(f\"Video Encoder (ResNet50): {Config.VIDEO_EMBEDDING_DIM}-dim output\") \n",
        "print(f\"Audio Encoder (1D CNN): {Config.AUDIO_EMBEDDING_DIM}-dim output\")\n",
        "print(f\"Fusion Network: {Config.TEXT_EMBEDDING_DIM + Config.VIDEO_EMBEDDING_DIM + Config.AUDIO_EMBEDDING_DIM} â†’ {Config.FUSION_DIM} â†’ {Config.FUSION_DIM//2} â†’ {Config.NUM_CLASSES}\")\n",
        "print(f\"Final Output: {Config.NUM_CLASSES} revenue classes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Dataset and Data Loading\n",
        "\n",
        "Creating the dataset class to handle multimodal movie data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MovieDataset(Dataset):\n",
        "    \"\"\"Dataset class for movie data with multimodal inputs\"\"\"\n",
        "    \n",
        "    def __init__(self, dataframe, tokenizer, video_processor, transform=None, mode='train'):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.video_processor = video_processor\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        \n",
        "        # Video transforms\n",
        "        self.video_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((Config.VIDEO_FRAME_SIZE, Config.VIDEO_FRAME_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        \n",
        "        print(f\"Dataset created with {len(self.df)} samples in {mode} mode\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        \n",
        "        # Text processing\n",
        "        text = str(row['Description']) if pd.notna(row['Description']) else \"\"\n",
        "        text_encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=Config.MAX_TEXT_LENGTH,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # Video processing\n",
        "        video_id = self.video_processor.extract_video_id(row['Trailer'])\n",
        "        frames = np.zeros((Config.FRAMES_PER_VIDEO, Config.VIDEO_FRAME_SIZE, \n",
        "                          Config.VIDEO_FRAME_SIZE, 3))\n",
        "        audio = np.zeros(Config.AUDIO_SAMPLE_RATE * Config.AUDIO_DURATION)\n",
        "        \n",
        "        if video_id and self.mode == 'train':  # Only process videos during training\n",
        "            try:\n",
        "                video_path = self.video_processor.download_video(row['Trailer'], video_id)\n",
        "                if video_path:\n",
        "                    extracted_frames = self.video_processor.extract_frames(video_path, Config.FRAMES_PER_VIDEO)\n",
        "                    extracted_audio, _ = self.video_processor.extract_audio(video_path)\n",
        "                    \n",
        "                    if extracted_frames is not None:\n",
        "                        frames = extracted_frames\n",
        "                    if extracted_audio is not None:\n",
        "                        audio = extracted_audio[:len(audio)]  # Truncate to desired length\n",
        "            except:\n",
        "                pass  # Use zero frames/audio if processing fails\n",
        "        \n",
        "        # Transform video frames\n",
        "        transformed_frames = []\n",
        "        for frame in frames:\n",
        "            if frame.max() > 1:  # If pixel values are in [0, 255]\n",
        "                frame = frame.astype(np.uint8)\n",
        "            else:  # If pixel values are in [0, 1]\n",
        "                frame = (frame * 255).astype(np.uint8)\n",
        "            transformed_frame = self.video_transform(frame)\n",
        "            transformed_frames.append(transformed_frame)\n",
        "        \n",
        "        video_tensor = torch.stack(transformed_frames)\n",
        "        audio_tensor = torch.FloatTensor(audio)\n",
        "        \n",
        "        # Target\n",
        "        target = row['y'] if 'y' in row else 0\n",
        "        \n",
        "        return {\n",
        "            'input_ids': text_encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': text_encoding['attention_mask'].squeeze(),\n",
        "            'video_frames': video_tensor,\n",
        "            'audio': audio_tensor,\n",
        "            'target': torch.LongTensor([target])[0]\n",
        "        }\n",
        "\n",
        "print(\"âœ… MovieDataset class created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Training and Evaluation Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelTrainer:\n",
        "    \"\"\"Training and evaluation utilities\"\"\"\n",
        "    \n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.model.to(device)\n",
        "        print(f\"Model moved to device: {device}\")\n",
        "    \n",
        "    def train_epoch(self, dataloader, optimizer, criterion):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        \n",
        "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "            video_frames = batch['video_frames'].to(self.device)\n",
        "            audio = batch['audio'].to(self.device)\n",
        "            target = batch['target'].to(self.device)\n",
        "            \n",
        "            outputs = self.model(input_ids, attention_mask, video_frames, audio)\n",
        "            loss = criterion(outputs, target)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            targets.extend(target.cpu().numpy())\n",
        "        \n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        accuracy = accuracy_score(targets, predictions)\n",
        "        return avg_loss, accuracy\n",
        "    \n",
        "    def evaluate(self, dataloader, criterion):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                video_frames = batch['video_frames'].to(self.device)\n",
        "                audio = batch['audio'].to(self.device)\n",
        "                target = batch['target'].to(self.device)\n",
        "                \n",
        "                outputs = self.model(input_ids, attention_mask, video_frames, audio)\n",
        "                loss = criterion(outputs, target)\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "                predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "                targets.extend(target.cpu().numpy())\n",
        "        \n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        metrics = self.calculate_metrics(targets, predictions)\n",
        "        return avg_loss, metrics, predictions, targets\n",
        "    \n",
        "    def calculate_metrics(self, y_true, y_pred):\n",
        "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n",
        "            'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
        "            'precision_weighted': precision_score(y_true, y_pred, average='weighted'),\n",
        "            'precision_macro': precision_score(y_true, y_pred, average='macro'),\n",
        "            'recall_weighted': recall_score(y_true, y_pred, average='weighted'),\n",
        "            'recall_macro': recall_score(y_true, y_pred, average='macro'),\n",
        "        }\n",
        "        return metrics\n",
        "    \n",
        "    def plot_confusion_matrix(self, y_true, y_pred, class_names):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        return cm\n",
        "\n",
        "print(\"âœ… ModelTrainer class created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7. Data Preparation and Training Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data(data_path, split_option='option1'):\n",
        "    \"\"\"Prepare and split the dataset\"\"\"\n",
        "    print(f\"Loading data from: {data_path}\")\n",
        "    \n",
        "    # Load data\n",
        "    df = pd.read_csv(data_path)\n",
        "    df = df.dropna(subset=['Description', 'Trailer', 'Verdict'])\n",
        "    \n",
        "    # Label mapping\n",
        "    df['y'] = df['Verdict'].map(LABEL_MAPPING)\n",
        "    df = df.dropna(subset=['y'])\n",
        "    \n",
        "    # Get split ratios\n",
        "    train_ratio, val_ratio, test_ratio = Config.SPLIT_OPTIONS[split_option]\n",
        "    print(f\"Using split option {split_option}: {train_ratio}-{val_ratio}-{test_ratio}\")\n",
        "    \n",
        "    # Split data\n",
        "    X = df.drop(['y', 'Verdict'], axis=1)\n",
        "    y = df['y']\n",
        "    \n",
        "    # First split: train vs (val + test)\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=(val_ratio + test_ratio), \n",
        "        random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Second split: val vs test\n",
        "    val_size = val_ratio / (val_ratio + test_ratio)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=(1 - val_size),\n",
        "        random_state=42, stratify=y_temp\n",
        "    )\n",
        "    \n",
        "    # Combine X and y back\n",
        "    train_df = pd.concat([X_train, y_train], axis=1)\n",
        "    val_df = pd.concat([X_val, y_val], axis=1)\n",
        "    test_df = pd.concat([X_test, y_test], axis=1)\n",
        "    \n",
        "    print(f\"Data split results:\")\n",
        "    print(f\"  Train: {len(train_df)} samples ({len(train_df)/len(df):.1%})\")\n",
        "    print(f\"  Validation: {len(val_df)} samples ({len(val_df)/len(df):.1%})\")\n",
        "    print(f\"  Test: {len(test_df)} samples ({len(test_df)/len(df):.1%})\")\n",
        "    \n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "def create_data_loaders(train_df, val_df, test_df, tokenizer, video_processor):\n",
        "    \"\"\"Create PyTorch data loaders\"\"\"\n",
        "    \n",
        "    train_dataset = MovieDataset(train_df, tokenizer, video_processor, mode='train')\n",
        "    val_dataset = MovieDataset(val_df, tokenizer, video_processor, mode='val')\n",
        "    test_dataset = MovieDataset(test_df, tokenizer, video_processor, mode='test')\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, \n",
        "                             shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, \n",
        "                           shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, \n",
        "                            shuffle=False, num_workers=2)\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "print(\"âœ… Data preparation functions created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8. Main Training Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, device, split_name=\"option1\"):\n",
        "    \"\"\"Train the multimodal model\"\"\"\n",
        "    \n",
        "    print(f\"ðŸš€ Starting training for {split_name}\")\n",
        "    \n",
        "    # Setup training\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=0.01)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
        "    \n",
        "    trainer = ModelTrainer(model, device)\n",
        "    \n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    \n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        print(f\"\\nðŸ“Š Epoch {epoch+1}/{Config.NUM_EPOCHS}\")\n",
        "        \n",
        "        # Training\n",
        "        train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, criterion)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_metrics, _, _ = trainer.evaluate(val_loader, criterion)\n",
        "        val_acc = val_metrics['accuracy']\n",
        "        \n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # Save metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        print(f\"Val F1 (weighted): {val_metrics['f1_weighted']:.4f}\")\n",
        "        print(f\"Val F1 (macro): {val_metrics['f1_macro']:.4f}\")\n",
        "        \n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f'best_multimodal_model_{split_name}.pth')\n",
        "            print(\"ðŸ’¾ Saved best model!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            \n",
        "        if patience_counter >= Config.PATIENCE:\n",
        "            print(f\"â¹ï¸ Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "    \n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(train_losses, label='Train', marker='o')\n",
        "    plt.plot(val_losses, label='Validation', marker='s')\n",
        "    plt.title(f'Training Loss - {split_name}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(train_accs, label='Train', marker='o')\n",
        "    plt.plot(val_accs, label='Validation', marker='s')\n",
        "    plt.title(f'Training Accuracy - {split_name}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(1, 3, 3)\n",
        "    lr_values = [scheduler.get_last_lr()[0]] * len(train_losses) if hasattr(scheduler, 'get_last_lr') else [Config.LEARNING_RATE] * len(train_losses)\n",
        "    plt.plot(lr_values, marker='o')\n",
        "    plt.title(f'Learning Rate - {split_name}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'training_curves_{split_name}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    return trainer\n",
        "\n",
        "print(\"âœ… Training function created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 9. Evaluation Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(trainer, test_loader, split_name=\"option1\"):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    \n",
        "    print(f\"ðŸ“ˆ Evaluating model on test set for {split_name}...\")\n",
        "    \n",
        "    # Load best model\n",
        "    trainer.model.load_state_dict(torch.load(f'best_multimodal_model_{split_name}.pth'))\n",
        "    \n",
        "    # Evaluate\n",
        "    test_loss, test_metrics, predictions, targets = trainer.evaluate(test_loader, nn.CrossEntropyLoss())\n",
        "    \n",
        "    # Print metrics\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ðŸŽ¯ TEST SET EVALUATION RESULTS - {split_name.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"ðŸ“Š MAIN METRICS:\")\n",
        "    print(f\"   Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"   Accuracy: {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']*100:.2f}%)\")\n",
        "    print(f\"   F1 Score (Weighted): {test_metrics['f1_weighted']:.4f}\")\n",
        "    print(f\"   F1 Score (Macro): {test_metrics['f1_macro']:.4f}\")\n",
        "    print(f\"   Precision (Weighted): {test_metrics['precision_weighted']:.4f}\")\n",
        "    print(f\"   Precision (Macro): {test_metrics['precision_macro']:.4f}\")\n",
        "    print(f\"   Recall (Weighted): {test_metrics['recall_weighted']:.4f}\")\n",
        "    print(f\"   Recall (Macro): {test_metrics['recall_macro']:.4f}\")\n",
        "    \n",
        "    # Classification report\n",
        "    class_names = list(LABEL_MAPPING.keys())\n",
        "    print(f\"\\nðŸ“ˆ DETAILED CLASSIFICATION REPORT:\")\n",
        "    print(classification_report(targets, predictions, target_names=class_names))\n",
        "    \n",
        "    # Enhanced confusion matrix visualization\n",
        "    print(f\"\\nðŸŽ¯ CONFUSION MATRIX ANALYSIS:\")\n",
        "    cm = confusion_matrix(targets, predictions)\n",
        "    \n",
        "    # Plot enhanced confusion matrix\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    \n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "               xticklabels=class_names, yticklabels=class_names,\n",
        "               cbar_kws={'label': 'Number of Samples'})\n",
        "    plt.title(f'Confusion Matrix - {split_name.upper()}', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Predicted Revenue Category', fontsize=12)\n",
        "    plt.ylabel('Actual Revenue Category', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save confusion matrix plot\n",
        "    plt.savefig(f'confusion_matrix_{split_name}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Print confusion matrix as numbers\n",
        "    print(f\"\\nConfusion Matrix Values:\")\n",
        "    print(\"Rows = Actual Revenue Category, Columns = Predicted Revenue Category\")\n",
        "    print(f\"{'':>12}\", end=\"\")\n",
        "    for name in class_names:\n",
        "        print(f\"{name[:8]:>8}\", end=\"\")\n",
        "    print()\n",
        "    \n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"{name[:12]:>12}\", end=\"\")\n",
        "        for j in range(len(class_names)):\n",
        "            print(f\"{cm[i,j]:>8}\", end=\"\")\n",
        "        print()\n",
        "    \n",
        "    # Per-class detailed metrics\n",
        "    print(f\"\\nðŸ“Š PER-CLASS DETAILED METRICS:\")\n",
        "    per_class_f1 = f1_score(targets, predictions, average=None)\n",
        "    per_class_precision = precision_score(targets, predictions, average=None, zero_division=0)\n",
        "    per_class_recall = recall_score(targets, predictions, average=None, zero_division=0)\n",
        "    \n",
        "    print(f\"{'Class':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
        "    print(\"-\" * 55)\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        support = np.sum(targets == i)\n",
        "        print(f\"{class_name:<15} {per_class_precision[i]:<10.3f} {per_class_recall[i]:<10.3f} {per_class_f1[i]:<10.3f} {support:<10}\")\n",
        "    \n",
        "    # Model performance summary\n",
        "    print(f\"\\nðŸ† PERFORMANCE SUMMARY:\")\n",
        "    print(f\"   Total Test Samples: {len(targets)}\")\n",
        "    print(f\"   Correct Predictions: {np.sum(targets == predictions)}\")\n",
        "    print(f\"   Incorrect Predictions: {np.sum(targets != predictions)}\")\n",
        "    print(f\"   Best Performing Class: {class_names[np.argmax(per_class_f1)]}\")\n",
        "    print(f\"   Worst Performing Class: {class_names[np.argmin(per_class_f1)]}\")\n",
        "    \n",
        "    # Save results\n",
        "    results = {\n",
        "        'split_name': split_name,\n",
        "        'test_metrics': test_metrics,\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'predictions': predictions,\n",
        "        'targets': targets,\n",
        "        'classification_report': classification_report(targets, predictions, \n",
        "                                                     target_names=class_names, \n",
        "                                                     output_dict=True)\n",
        "    }\n",
        "    \n",
        "    # Save to JSON\n",
        "    with open(f'evaluation_results_{split_name}.json', 'w') as f:\n",
        "        # Convert numpy arrays to lists for JSON serialization\n",
        "        serializable_results = {\n",
        "            'split_name': results['split_name'],\n",
        "            'test_metrics': results['test_metrics'],\n",
        "            'confusion_matrix': results['confusion_matrix'],\n",
        "            'predictions': [int(x) for x in results['predictions']],\n",
        "            'targets': [int(x) for x in results['targets']],\n",
        "            'classification_report': results['classification_report']\n",
        "        }\n",
        "        json.dump(serializable_results, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nðŸ’¾ Files saved:\")\n",
        "    print(f\"   ðŸ“„ evaluation_results_{split_name}.json\")\n",
        "    print(f\"   ðŸ“Š confusion_matrix_{split_name}.png\")\n",
        "    print(f\"\\nâœ… Evaluation completed successfully!\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"âœ… Evaluation function created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 10. Complete Training Pipeline\n",
        "\n",
        "Now let's run the complete training and evaluation pipeline for all three data split options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ðŸ”§ Using device: {device}\")\n",
        "\n",
        "# Initialize components\n",
        "print(\"\\nðŸš€ Initializing components...\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "video_processor = YouTubeVideoProcessor()\n",
        "\n",
        "# Data path - update this to your dataset path\n",
        "data_path = 'Data/TMRDB.csv'  # Update this path\n",
        "\n",
        "print(\"\\nâœ… All components initialized successfully!\")\n",
        "print(\"Ready to start training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main training loop for all split options\n",
        "def run_complete_training():\n",
        "    \"\"\"Run training for all three data split options\"\"\"\n",
        "    \n",
        "    print(\"ðŸŽ¬ MULTIMODAL MOVIE REVENUE PREDICTION SYSTEM\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    # Test all three split options\n",
        "    for split_option in ['option1', 'option2', 'option3']:\n",
        "        print(f\"\\n{'='*30} {split_option.upper()} {'='*30}\")\n",
        "        \n",
        "        try:\n",
        "            # Prepare data\n",
        "            train_df, val_df, test_df = prepare_data(data_path, split_option)\n",
        "            \n",
        "            # Create data loaders\n",
        "            train_loader, val_loader, test_loader = create_data_loaders(\n",
        "                train_df, val_df, test_df, tokenizer, video_processor\n",
        "            )\n",
        "            \n",
        "            # Initialize fresh model for each split\n",
        "            model = MultimodalFusionModel()\n",
        "            print(f\"\\nðŸ“Š Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "            \n",
        "            # Train model\n",
        "            trainer = train_model(model, train_loader, val_loader, device, split_option)\n",
        "            \n",
        "            # Evaluate model\n",
        "            results = evaluate_model(trainer, test_loader, split_option)\n",
        "            all_results[split_option] = results\n",
        "            \n",
        "            print(f\"\\nâœ… Completed {split_option}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error in {split_option}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Compare results across splits\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"ðŸ“Š COMPARISON ACROSS DIFFERENT DATA SPLITS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    comparison_data = []\n",
        "    for split_option, results in all_results.items():\n",
        "        ratios = Config.SPLIT_OPTIONS[split_option]\n",
        "        comparison_data.append({\n",
        "            'Split': f\"{ratios[0]*100:.0f}-{ratios[1]*100:.0f}-{ratios[2]*100:.0f}\",\n",
        "            'Accuracy': f\"{results['test_metrics']['accuracy']:.4f}\",\n",
        "            'F1 (Weighted)': f\"{results['test_metrics']['f1_weighted']:.4f}\",\n",
        "            'F1 (Macro)': f\"{results['test_metrics']['f1_macro']:.4f}\",\n",
        "            'Precision (W)': f\"{results['test_metrics']['precision_weighted']:.4f}\",\n",
        "            'Recall (W)': f\"{results['test_metrics']['recall_weighted']:.4f}\"\n",
        "        })\n",
        "    \n",
        "    if comparison_data:\n",
        "        comparison_df = pd.DataFrame(comparison_data)\n",
        "        print(comparison_df.to_string(index=False))\n",
        "        \n",
        "        # Save comparison\n",
        "        comparison_df.to_csv('split_comparison.csv', index=False)\n",
        "        print(\"\\nðŸ’¾ Comparison saved to 'split_comparison.csv'\")\n",
        "    \n",
        "    print(\"\\nðŸŽ‰ Training completed! Generated files:\")\n",
        "    for split_option in all_results.keys():\n",
        "        print(f\"   - best_multimodal_model_{split_option}.pth\")\n",
        "        print(f\"   - evaluation_results_{split_option}.json\")\n",
        "        print(f\"   - training_curves_{split_option}.png\")\n",
        "    print(\"   - split_comparison.csv\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# To run the complete training, uncomment the line below:\n",
        "# all_results = run_complete_training()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 11. Quick Demo/Test (Optional)\n",
        "\n",
        "You can run this section to test individual components or train on a smaller sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Test Confusion Matrix and Evaluation Display\n",
        "def demo_confusion_matrix():\n",
        "    \"\"\"Demo function to test confusion matrix and evaluation display\"\"\"\n",
        "    print(\"ðŸŽ¯ DEMO: Testing Confusion Matrix and Evaluation Display\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Create dummy predictions and targets for demonstration\n",
        "    np.random.seed(42)\n",
        "    n_samples = 100\n",
        "    n_classes = 8\n",
        "    \n",
        "    # Generate dummy targets and predictions\n",
        "    targets = np.random.randint(0, n_classes, n_samples)\n",
        "    predictions = np.random.randint(0, n_classes, n_samples)\n",
        "    \n",
        "    # Make some predictions correct for realistic metrics\n",
        "    correct_mask = np.random.random(n_samples) < 0.6  # 60% accuracy\n",
        "    predictions[correct_mask] = targets[correct_mask]\n",
        "    \n",
        "    class_names = list(LABEL_MAPPING.keys())\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(targets, predictions)\n",
        "    f1_weighted = f1_score(targets, predictions, average='weighted')\n",
        "    f1_macro = f1_score(targets, predictions, average='macro')\n",
        "    precision_weighted = precision_score(targets, predictions, average='weighted')\n",
        "    recall_weighted = recall_score(targets, predictions, average='weighted')\n",
        "    \n",
        "    # Print metrics\n",
        "    print(f\"ðŸ“Š EVALUATION METRICS:\")\n",
        "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"   F1 Score (Weighted): {f1_weighted:.4f}\")\n",
        "    print(f\"   F1 Score (Macro): {f1_macro:.4f}\")\n",
        "    print(f\"   Precision (Weighted): {precision_weighted:.4f}\")\n",
        "    print(f\"   Recall (Weighted): {recall_weighted:.4f}\")\n",
        "    \n",
        "    # Classification report\n",
        "    print(f\"\\nðŸ“ˆ CLASSIFICATION REPORT:\")\n",
        "    print(classification_report(targets, predictions, target_names=class_names))\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    print(f\"\\nðŸŽ¯ CONFUSION MATRIX:\")\n",
        "    cm = confusion_matrix(targets, predictions)\n",
        "    \n",
        "    # Display confusion matrix as heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "               xticklabels=class_names, yticklabels=class_names,\n",
        "               cbar_kws={'label': 'Number of Samples'})\n",
        "    plt.title('Confusion Matrix - Demo', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Predicted Revenue Category', fontsize=12)\n",
        "    plt.ylabel('Actual Revenue Category', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Display confusion matrix as numbers\n",
        "    print(f\"\\nConfusion Matrix (Numbers):\")\n",
        "    print(\"Rows = Actual, Columns = Predicted\")\n",
        "    print(f\"{'':>12}\", end=\"\")\n",
        "    for name in class_names:\n",
        "        print(f\"{name[:8]:>8}\", end=\"\")\n",
        "    print()\n",
        "    \n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"{name[:12]:>12}\", end=\"\")\n",
        "        for j in range(len(class_names)):\n",
        "            print(f\"{cm[i,j]:>8}\", end=\"\")\n",
        "        print()\n",
        "    \n",
        "    # Per-class metrics\n",
        "    print(f\"\\nðŸ“Š PER-CLASS DETAILED METRICS:\")\n",
        "    per_class_f1 = f1_score(targets, predictions, average=None)\n",
        "    per_class_precision = precision_score(targets, predictions, average=None)\n",
        "    per_class_recall = recall_score(targets, predictions, average=None)\n",
        "    \n",
        "    print(f\"{'Class':<12} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
        "    print(\"-\" * 52)\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        support = np.sum(targets == i)\n",
        "        print(f\"{class_name:<12} {per_class_precision[i]:<10.3f} {per_class_recall[i]:<10.3f} {per_class_f1[i]:<10.3f} {support:<10}\")\n",
        "    \n",
        "    print(f\"\\nâœ… Demo completed! All visualization functions are working.\")\n",
        "    \n",
        "    return cm, accuracy, f1_weighted\n",
        "\n",
        "# Run the demo\n",
        "print(\"ðŸš€ Running confusion matrix demo...\")\n",
        "demo_cm, demo_acc, demo_f1 = demo_confusion_matrix()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Test Function to Display Output Values\n",
        "def test_evaluation_display():\n",
        "    \"\"\"Test function to show evaluation metrics and confusion matrix display\"\"\"\n",
        "    print(\"ðŸ§ª TESTING EVALUATION DISPLAY FUNCTIONS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Create a ModelTrainer instance for testing\n",
        "    model = MultimodalFusionModel()\n",
        "    device = torch.device('cpu')\n",
        "    trainer = ModelTrainer(model, device)\n",
        "    \n",
        "    # Generate dummy test data\n",
        "    np.random.seed(42)\n",
        "    n_samples = 80\n",
        "    targets = np.random.randint(0, 8, n_samples)\n",
        "    predictions = np.random.randint(0, 8, n_samples)\n",
        "    \n",
        "    # Make some predictions match targets for realistic accuracy\n",
        "    correct_indices = np.random.choice(n_samples, size=int(0.65 * n_samples), replace=False)\n",
        "    predictions[correct_indices] = targets[correct_indices]\n",
        "    \n",
        "    class_names = list(LABEL_MAPPING.keys())\n",
        "    \n",
        "    # Test the actual plot_confusion_matrix function from ModelTrainer\n",
        "    print(\"ðŸ“Š Testing ModelTrainer.plot_confusion_matrix function:\")\n",
        "    cm = trainer.plot_confusion_matrix(targets, predictions, class_names)\n",
        "    \n",
        "    # Calculate and display metrics manually \n",
        "    accuracy = accuracy_score(targets, predictions)\n",
        "    f1_weighted = f1_score(targets, predictions, average='weighted')\n",
        "    f1_macro = f1_score(targets, predictions, average='macro')\n",
        "    \n",
        "    print(f\"\\nðŸ“ˆ SAMPLE EVALUATION METRICS:\")\n",
        "    print(f\"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"   F1 Score (Weighted): {f1_weighted:.4f}\")\n",
        "    print(f\"   F1 Score (Macro): {f1_macro:.4f}\")\n",
        "    \n",
        "    # Test the evaluate function from ModelTrainer\n",
        "    print(f\"\\nðŸ” Testing ModelTrainer.calculate_metrics function:\")\n",
        "    test_metrics = trainer.calculate_metrics(targets, predictions)\n",
        "    \n",
        "    for metric, value in test_metrics.items():\n",
        "        print(f\"   {metric}: {value:.4f}\")\n",
        "    \n",
        "    print(f\"\\nâœ… All display functions are working correctly!\")\n",
        "    print(f\"ðŸŽ¯ You should see:\")\n",
        "    print(f\"   â€¢ A confusion matrix heatmap above\")\n",
        "    print(f\"   â€¢ Numerical accuracy and F1 scores\")\n",
        "    print(f\"   â€¢ All evaluation metrics\")\n",
        "    \n",
        "    return targets, predictions, cm\n",
        "\n",
        "# Run the test\n",
        "test_targets, test_predictions, test_cm = test_evaluation_display()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ðŸ“Š Troubleshooting Visualization Issues\n",
        "\n",
        "If you cannot see the confusion matrix or evaluation outputs, try these solutions:\n",
        "\n",
        "### 1. **Ensure Matplotlib Backend is Set Correctly**\n",
        "```python\n",
        "import matplotlib\n",
        "matplotlib.use('inline')  # For Jupyter notebooks\n",
        "plt.rcParams['figure.figsize'] = [12, 8]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "```\n",
        "\n",
        "### 2. **Force Display with explicit show()**\n",
        "```python\n",
        "# After any plotting command, add:\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 3. **Check Jupyter Magic Commands**\n",
        "Make sure you have this at the top of your notebook:\n",
        "```python\n",
        "%matplotlib inline\n",
        "```\n",
        "\n",
        "### 4. **Alternative: Save and Display Images**\n",
        "If plots still don't show, the functions now save images to files:\n",
        "- `confusion_matrix_[split_name].png`\n",
        "- `training_curves_[split_name].png`\n",
        "\n",
        "### 5. **Test Display Function**\n",
        "Run the test functions above to verify everything works:\n",
        "- `demo_confusion_matrix()` - Shows example confusion matrix\n",
        "- `test_evaluation_display()` - Tests ModelTrainer functions\n",
        "\n",
        "### 6. **Check for Output Suppression**\n",
        "If running programmatically, outputs might be suppressed. Make sure to:\n",
        "- Run cells individually to see outputs\n",
        "- Check that print statements are executing\n",
        "- Look for saved files in your directory\n",
        "\n",
        "### What You Should See:\n",
        "âœ… **Confusion Matrix**: Heatmap with 8x8 grid (revenue categories)  \n",
        "âœ… **Metrics**: Accuracy, F1, Precision, Recall values  \n",
        "âœ… **Classification Report**: Per-class performance breakdown  \n",
        "âœ… **Performance Summary**: Best/worst performing classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test with dummy data\n",
        "def quick_test():\n",
        "    \"\"\"Quick test of the model architecture with dummy data\"\"\"\n",
        "    print(\"ðŸ§ª Running quick test with dummy data...\")\n",
        "    \n",
        "    # Create dummy inputs\n",
        "    batch_size = 2\n",
        "    dummy_text_ids = torch.randint(0, 1000, (batch_size, Config.MAX_TEXT_LENGTH))\n",
        "    dummy_attention_mask = torch.ones(batch_size, Config.MAX_TEXT_LENGTH)\n",
        "    dummy_video = torch.randn(batch_size, Config.FRAMES_PER_VIDEO, 3, Config.VIDEO_FRAME_SIZE, Config.VIDEO_FRAME_SIZE)\n",
        "    dummy_audio = torch.randn(batch_size, Config.AUDIO_SAMPLE_RATE * Config.AUDIO_DURATION)\n",
        "    \n",
        "    # Test model forward pass\n",
        "    model = MultimodalFusionModel()\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(dummy_text_ids, dummy_attention_mask, dummy_video, dummy_audio)\n",
        "    \n",
        "    print(f\"âœ… Model test successful!\")\n",
        "    print(f\"   Input shapes:\")\n",
        "    print(f\"     Text: {dummy_text_ids.shape}\")\n",
        "    print(f\"     Video: {dummy_video.shape}\")\n",
        "    print(f\"     Audio: {dummy_audio.shape}\")\n",
        "    print(f\"   Output shape: {outputs.shape}\")\n",
        "    print(f\"   Predicted classes: {torch.argmax(outputs, dim=1).numpy()}\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "# Run quick test\n",
        "test_result = quick_test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 12. Usage Instructions\n",
        "\n",
        "### To train the complete system:\n",
        "\n",
        "1. **Update the data path** in cell 28 to point to your `TMRDB.csv` file\n",
        "2. **Uncomment the training line** in cell 29: `# all_results = run_complete_training()`\n",
        "3. **Run all cells** in sequence\n",
        "\n",
        "### What the system does:\n",
        "\n",
        "- **Downloads YouTube trailers** automatically\n",
        "- **Extracts video frames** using OpenCV\n",
        "- **Extracts audio** using librosa\n",
        "- **Processes text** using BERT\n",
        "- **Trains multimodal fusion** model\n",
        "- **Evaluates on all metrics** (accuracy, F1, precision, recall, confusion matrix)\n",
        "- **Tests 3 different data splits** (70-20-10, 75-15-10, 80-10-10)\n",
        "\n",
        "### Output files generated:\n",
        "\n",
        "- `best_multimodal_model_[split].pth` - Trained model weights\n",
        "- `evaluation_results_[split].json` - Detailed metrics\n",
        "- `training_curves_[split].png` - Training visualizations  \n",
        "- `split_comparison.csv` - Performance comparison\n",
        "- `confusion_matrix.png` - Confusion matrix plots\n",
        "\n",
        "### Model Architecture Summary:\n",
        "\n",
        "```\n",
        "ðŸ“Š Total Parameters: ~140M\n",
        "ðŸ§  Text Encoder (BERT): 768-dim â†’ Revenue Classes\n",
        "ðŸŽ¬ Video Encoder (ResNet50): 2048-dim â†’ Revenue Classes  \n",
        "ðŸŽµ Audio Encoder (1D CNN): 1024-dim â†’ Revenue Classes\n",
        "ðŸ”— Fusion Network: 3840-dim â†’ 512 â†’ 256 â†’ 8 classes\n",
        "```\n",
        "\n",
        "### Revenue Categories:\n",
        "0. Disaster, 1. Flop, 2. Successful, 3. Average\n",
        "4. Hit, 5. Outstanding, 6. Superhit, 7. Blockbuster\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
